{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap=\"Reds\"):\n",
    "\td2l.use_svg_display()\n",
    "\tnum_rows, num_cols = matrices.shape[0], matrices.shape[1]\n",
    "\tfig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize, sharex=True, sharey=True, squeeze=False) # shape0, shape1 -- number of subplots\n",
    "\n",
    "\tfor i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)): # row_axes seperate out first dimension -- (1, 1, 10, 10) to (1, 10, 10)\n",
    "\t\tfor j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)): # ax seperate out second dimension -- (1, 10, 10) to (10, 10)\n",
    "\t\t\tpcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)\n",
    "\t\t\tif i == num_rows - 1:\n",
    "\t\t\t\tax.set_xlabel(xlabel) # we may have (row, col) subplots -- we set xlabel at once at the bottom\n",
    "\t\t\tif j == 0:\n",
    "\t\t\t\tax.set_ylabel(ylabel)\n",
    "\t\t\tif titles:\n",
    "\t\t\t\tax.set_title(titles[j]) # a list a subtitles\n",
    "\tfig.colorbar(pcm, ax=axes, shrink=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: (5, 10)\n",
    "# value: [1, 5, 3, 6, 2] -- matching the rows \n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "\t\"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "\n",
    "\tmaxlen = X.size(1)\n",
    "\tmask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "\t#print(mask)\n",
    "\n",
    "\tX[~mask] = value # set the masked elements to value \n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked softmax operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: (2, 3, 4) -- attention weights\n",
    "# valid lens: specify how many elements to mask in each minibatch \n",
    "# valid lens (high dim): specify how to mask each minibatch \n",
    "def masked_softmax(X, valid_lens):\n",
    "\t\"\"\"Perform softmax operation by masking elements on the last axis\"\"\"\n",
    "\n",
    "\tif valid_lens == None:\n",
    "\t\treturn nn.functional.softmax(X, dim=-1)\n",
    "\telse:\n",
    "\t\tshape = X.shape\n",
    "\t\tif valid_lens.dim() == 1:\n",
    "\t\t\t# \n",
    "\t\t\tvalid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "\t\telse:\n",
    "\t\t\tvalid_lens = valid_lens.reshape(-1)\n",
    "\t\tX = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "\n",
    "\t\treturn nn.functional.softmax(X.reshape(shape), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\t\"\"\"Position encoding X+P with dropout\"\"\"\n",
    "\n",
    "\tdef __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "\t\tsuper(PositionalEncoding, self).__init__()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\t# create a single batch_size 1, long enough P, longer than sequence length\n",
    "\t\tself.P = torch.zeros((1, max_len, num_hiddens))\n",
    "\t\tX = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32)/num_hiddens)\n",
    "\t\tself.P[:, :, 0::2] = torch.sin(X)\n",
    "\t\tself.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tX = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "\t\treturn self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "\t\"\"\"\"Scaled dot product attention with dropout\"\"\"\n",
    "\n",
    "\tdef __init__(self, dropout, **kwargs):\n",
    "\t\tsuper(DotProductAttention, self).__init__(**kwargs)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t\t# query: (batch_size, no. of queries, 'd')\n",
    "\t\t# key: (batch_size, no. of key-value pairs, 'd')\n",
    "\t\t# value: (batch_size, no. of key-value pairs, d_value)\n",
    "\t\t# valid_lens: (batch_size, ) or (batch_size, no. of queries)\n",
    "\t\n",
    "\tdef forward(self, queries, keys, values, valid_lens=None):\n",
    "\t\td = queries.shape[-1]\n",
    "\n",
    "\t\tscores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "\t\tself.attention_weights = masked_softmax(scores, valid_lens)\n",
    "\t\treturn torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "\t\"\"\"Transposition for parallel computation of multiple attention heads\"\"\"\n",
    "\n",
    "\tX = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\tX = X.permute(0, 2, 1, 3)\n",
    "\n",
    "\treturn X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "\t\"\"\"Reverse the operation of transpose_qkv\"\"\"\n",
    "\n",
    "\tX = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "\tX = X.permute(0, 2, 1, 3)\n",
    "\n",
    "\treturn X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_lens: (batch_size, ) or (batch_size, no. of queries)\n",
    "# Note: the mask (valid_len) used here, I copy it num_heads times\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\t\"\"\"Multi-head attention\"\"\"\n",
    "\n",
    "\tdef __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "\t\tsuper(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\t\tself.num_heads = num_heads\n",
    "\t\tself.attention = DotProductAttention(dropout)\n",
    "\t\tself.W_q = nn.Linear(query_size, num_hiddens, bias=use_bias) # (in_feature, out_feature)\n",
    "\t\tself.W_k = nn.Linear(key_size, num_hiddens, bias=use_bias)\n",
    "\t\tself.W_v = nn.Linear(value_size, num_hiddens, bias=use_bias)\n",
    "\t\tself.W_o = nn.Linear(num_hiddens, num_hiddens, bias=use_bias)\n",
    "\t\n",
    "\tdef forward(self, queries, keys, values, valid_lens):\n",
    "\t\t\n",
    "\t\t# queries, keys, values:\n",
    "\t\t# (batch_size, no. of queries / keys/ values, num_hiddens) ---------------------note here we choose queries_size, value_size, key_size to be num_hiddens\n",
    "\t\t# valid_lens: (batch_size, ) or (batch_size, no. of queries)\n",
    "\n",
    "\t\tqueries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "\t\tkeys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "\t\tvalues = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "\n",
    "\t\tif valid_lens is not None:\n",
    "\t\t\tvalid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\t\t\n",
    "\t\tprint()\n",
    "\t\toutput = self.attention(queries, keys, values, valid_lens)\n",
    "\n",
    "\t\toutput_concat = transpose_output(output, self.num_heads)\n",
    "\n",
    "\t\treturn self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positionwise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "\t\"\"\"Positionwise feed-forward network\"\"\"\n",
    "\n",
    "\tdef __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "\t\tsuper(PositionWiseFFN, self).__init__(**kwargs)\n",
    "\t\tself.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\treturn self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual connection and layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (batch_size, seq_length, features_dim)\n",
    "# normalized shape is input.size()[1: ]\n",
    "# the normalizing direction is features_dim -- nomalizing along second dimension\n",
    "\n",
    "# X is input \n",
    "# Y is Multiattention(X)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "\t\"\"\"Residual connection followed by layer normalization with dropout implementation\"\"\"\n",
    "\n",
    "\tdef __init__(self, normalized_shape, dropout, **kwargs):\n",
    "\t\tsuper(AddNorm, self).__init__(**kwargs)\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\t\tself.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "\tdef forward(self, X, Y): \n",
    "\t\treturn self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\t\"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "\tdef __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs):\n",
    "\t\tsuper(EncoderBlock, self).__init__(**kwargs)\n",
    "\t\tself.attention = MultiHeadAttention(key_size=key_size, query_size=query_size, value_size=value_size, num_hiddens=num_hiddens, num_heads=num_heads, dropout=dropout, use_bias=use_bias)\n",
    "\t\tself.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "\t\tself.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "\t\tself.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "\tdef forward(self, X, valid_lens):\n",
    "\t\tY = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "\t\treturn self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now stack the blocks of encoders\n",
    "# we also need to rescale to learnable input embeddings by sqrt(embedding dim) to [-1, 1]. This is because each value of positional embeddding is [-1, 1]\n",
    "# num_layers: number of blocks of encoder\n",
    "# vocab_size: size of vocabulary dictionary\n",
    "\n",
    "class TransformerEncoder(d2l.Encoder):\n",
    "\t\"\"\"Transformer encoder.\"\"\"\n",
    "\n",
    "\tdef __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "\t\tsuper(TransformerEncoder, self).__init__(**kwargs)\n",
    "\t\tself.num_hiddens = num_hiddens\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, num_hiddens) # here we randomly initialize a input embedding matrix \n",
    "\t\tself.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "\t\tself.blks = nn.Sequential()\n",
    "\t\tfor i in range(num_layers):\n",
    "\t\t\tself.blks.add_module(\"Block\"+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=use_bias))\n",
    "\t\n",
    "\tdef forward(self, X, valid_lens, *args):\n",
    "\t\t\n",
    "\t\tX = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "\t\tself.attention_weights = [None] * len(self.blks) # create a attention weight list to hold attention weights at each block\n",
    "\t\tfor i, blk in enumerate(self.blks):\n",
    "\t\t\tX = blk(X, valid_lens)\n",
    "\t\t\tself.attention_weights[i] = blk.attention.attention.attention_weights # the actual attention weights is stored in the ScaledDotAttention\n",
    "\t\t\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.i stores all previous representations\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\t\"\"\"The ith decoder block\"\"\"\n",
    "\tdef __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "\t\tsuper(DecoderBlock, self).__init__(**kwargs)\n",
    "\t\tself.i = i\n",
    "\t\tself.attention1 = MultiHeadAttention(key_size=key_size, query_size=query_size, value_size=value_size, num_hiddens=num_hiddens, num_heads=num_heads, dropout=dropout)\n",
    "\t\tself.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "\t\tself.attention2 = MultiHeadAttention(key_size=key_size, query_size=query_size, value_size=value_size, num_hiddens=num_hiddens, num_heads=num_heads, dropout=dropout)\n",
    "\t\tself.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "\t\tself.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "\t\tself.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "\tdef forward(self, X, state):\n",
    "\t\t# state: (encoder_output, encoder_valid_len, some info)\n",
    "\t\tenc_outputs, enc_valid_lens = state[0], state[1]\n",
    "\n",
    "\t\tif state[2][self.i] is None: # if nothing is stored previously, we use all X. This only occurs during training\n",
    "\t\t\tkey_values = X\n",
    "\t\telse:\n",
    "\t\t\tkey_values = torch.cat((state[2][self.i], X), axis=1) # we combine the previous with current X. This often occurs during prediction ------------------- ?potential issues\n",
    "\t\tstate[2][self.i] = key_values\n",
    "\n",
    "\t\tif self.training: # True if training procedure applied\n",
    "\t\t\tbatch_size, num_steps, _ = X.shape # (batch_size, previous_timesteps, hidden_dimension)\n",
    "\t\t\tdec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1) # repeat to size (2, 1), this specifies how we mask each row in each minibatch\n",
    "\t\telse:\n",
    "\t\t\tdec_valid_lens = None # for prediction, we do not need mask, since we do not know what is after\n",
    "\t\t\n",
    "\t\t# main model\n",
    "\t\t# q, k, v as defined\n",
    "\t\tX2 = self.attention1(X, key_values, key_values, dec_valid_lens) # training: all X, but we mask all after, prediction: queries=current timestep repr, keys_values, all previous \n",
    "\t\tY = self.addnorm1(X, X2)\n",
    "\t\tY2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) # in the case encoder is masked \n",
    "\t\tZ = self.addnorm2(Y, Y2)\n",
    "\n",
    "\t\treturn self.addnorm3(Z, self.ffn(Z)), state # state here is passed as it was \n",
    "\n",
    "\t# some additional notes: \n",
    "\t# decoder consists of 2 functionality: training and prediction \n",
    "\n",
    "\t# during training: we use the whole sequence but mask attention weights\n",
    "\t# The attention weights matrix has dimension (q, k) -- the second query will only get acess to the first 2 key\n",
    "\t# state[2] is [None]\n",
    "\n",
    "\t# during prediction: we only have current embedding -- 1 query \n",
    "\t# the key-values combine current and all previous key-valuee to search for -- decoder self attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "\n",
    "\tdef __init__(self, vocab_size,  key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs):\n",
    "\t\tsuper(TransformerDecoder, self).__init__(**kwargs)\n",
    "\t\tself.num_hiddens = num_hiddens\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "\t\tself.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "\t\tself.blks = nn.Sequential()\n",
    "\t\tfor i in range(num_layers):\n",
    "\t\t\tself.blks.add_module(\"block\"+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i))\n",
    "\t\tself.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\t\t# prediction: we have (1, num_hiddens) --> (1, num_vocab_size)\n",
    "\n",
    "\tdef init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "\t\treturn [enc_outputs, enc_valid_lens, [None]*self.num_layers]\n",
    "\n",
    "\tdef forward(self, X, state):\n",
    "\t\tX = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n",
    "\t\tself._attention_weights = [[None] * len(self.blks) for _ in range(2)] # here we record attention from 2 attention module (2, i)\n",
    "\t\tfor i, blk in enumerate(self.blks):\n",
    "\n",
    "\t\t\tX, state = blk(X, state)\n",
    "\t\t\t# Decoder self-attention weights\n",
    "\t\t\tself._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "\t\t\t# Encoder-decoder attention weights\n",
    "\t\t\tself._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "\t\t\n",
    "\t\treturn self.dense(X), state\n",
    "\n",
    "\tdef attention_weights(self):\n",
    "\t\treturn self._attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder to Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)  # feature representation from encoder\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args) # use the feature representation from encoder to process a decoder init: (training--None), (prediction -- previous important information)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The softmax cross-entropy loss with mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "\t\"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "\t# 'pred' shape: ('batch_size', 'num_steps', 'vocab_size')  ---> shape should be (0, 2, 1)\n",
    "\t# 'label' shape: ('batch_size', num_steps) --- each number is a number between 0 and vocab_size/prob\n",
    "\t# 'valid_len' shape: ('batch_size', )\n",
    "\n",
    "\t# \"none\" return same shape as 'label'\n",
    "\t\n",
    "\t# num_steps have to be the same \n",
    "\t# during training, our labeled/pred length can be very long. We can restrict the length of each batch.\n",
    "\t# we ignore the irrelevant part\n",
    "\tdef forward(self, pred, label, valid_len):\n",
    "\t\tweights = torch.ones_like(label)\n",
    "\t\tweights = sequence_mask(weights, valid_len)\n",
    "\t\tself.reduction = 'none'\n",
    "\t\tunweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "\t\tweighted_loss = (unweighted_loss * weights).mean(dim=1) # mean of each batch along num_steps\n",
    "\t\treturn weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we enforce the first word <bos>\n",
    "\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                               device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing -------> the first word at all batch is <bos>\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()  # Make the loss scalar for `backward`\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
    "    # Set `net` to eval mode for inference\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # Add the batch axis\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "\n",
    "\n",
    "    # Add the batch axis\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # We use the token with the highest prediction likelihood as the input\n",
    "        # of the decoder at the next time step\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # Save attention weights (to be covered later)\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights())\n",
    "        # Once the end-of-sequence token is predicted, the generation of the\n",
    "        # output sequence is complete\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: small languague dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters of model\n",
    "\n",
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32] # 32 corresponds to the dim of such number to normalize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 92,  70,  32,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [114,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 186,  66,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [144,   9,  84,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [173,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94, 133,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   8,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94, 101,   7,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [194, 135, 168,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [156,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  53, 101, 129,   3,   5,   6,   6,   6],\n",
      "        [ 90,  53, 143,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 92,   7,  70, 131, 143,   1,   5,   6,   6,   6],\n",
      "        [162,  86,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [156,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [173,   9,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 175,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [102, 174,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 100, 187,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [120,   7,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 168,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  66,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [127,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7, 179, 118,  53, 146,   3,   5,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [177, 179,  34,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 17, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [182, 110,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 15,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 16,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [164,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 24,  78,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  32,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  29,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 93,  83,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,  31,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 46, 180, 173,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 175,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 126,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [120,  82,  28,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [120,   7,  65,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 60,   1,   5,   6,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [151,  43, 186,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [193,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [183,  43, 115,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 149,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 95, 165,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [177, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 167,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 49,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70, 112,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [173,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [180, 169,   7,  80,   1,   5,   6,   6,   6,   6],\n",
      "        [ 18,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [173,   9, 132,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   8,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   8,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70, 153,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 95, 158,  80,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [120,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [120,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [194, 171,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  66,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [ 77,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [180, 104,  42,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 189,   0, 154,   1,   5,   6,   6,   6,   6],\n",
      "        [120,  82,  27,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  53, 190,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 25,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  69,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 20,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [164,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [144,   7,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [144,   8,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [185,  43, 120,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 131,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [194, 135,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,   7, 174,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  32,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 15, 104,  42,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 190,   1,   5,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[148,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [180,  24, 107,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  72,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [172,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [123,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  71,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  24,  78,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 95,  74,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [170,  32,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 13,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36, 115, 144, 100,  76,   3,   5,   6,   6,   6],\n",
      "        [164,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  81,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [193, 180,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36, 131,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 45,  36,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 39,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 109, 165,  29,   7,   3,   5,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [151, 189,   0, 101, 111,   3,   5,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [162,  37,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [183,  30,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [131,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [185,  43, 115,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [173,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 186,  66,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [183,  91,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 46, 173,   7,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 60,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  71,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  57,   7,   7,   3,   5,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 169, 139,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6]])\n",
      "tensor([[  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 173,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  87,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [162,  98,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [170,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [144,   7,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [186,   7, 133,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 15,  24, 107,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [183,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 135,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [103, 174,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99, 132,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,  76, 161,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36, 101,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  29,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 67,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 104,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [127,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  44,  10,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [137,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 186,  66, 139,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 95, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [113,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 19,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [138, 130,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 18,  91,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 112,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 112,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 93, 122,  84,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [124,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 179, 118,  53, 146,   3,   5,   6,   6,   6],\n",
      "        [  7, 104,  78,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [155,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 15,  56,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 63,  83,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  65,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [166,   7,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  81,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [144,  70, 184,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [185,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 29,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   1,   5,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[ 49, 189,   0, 154,   1,   5,   6,   6,   6,   6],\n",
      "        [ 96, 165,  73,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  48,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 23, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [164,  37,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 45,  36,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [120,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  84,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 18,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 74, 179,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 22, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 128,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 95, 165,  41,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 110,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  98,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,  50, 131,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [164, 191,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [177, 179,  64,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 158,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,  54,   3,   5,   6,   6,   6],\n",
      "        [ 55, 189,   0, 173,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [123,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 125,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [182,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [147,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 19,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [180,   7,   7,   7,   7,   7,   1,   5,   6,   6],\n",
      "        [ 96, 165, 112,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [172, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 47,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,  66,   7,   3,   5,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 21, 179, 134,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 94, 104,   7,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 92,  51,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 99,  84,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 186,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 21,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 22, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [150,  43,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 16,   1,   5,   6,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [180,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [141, 142, 157,   7,   1,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  88,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  47,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [119,  55, 131,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 106,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  84,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [166,  88,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 62,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [105,   7,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [141, 142, 157,   7,   1,   5,   6,   6,   6,   6],\n",
      "        [164,  98,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 45,  36,  85,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 45,  36,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [144,  70,   7,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 129,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 100,  76,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [173,  70, 184,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 97,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [163,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 174,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [159,   2, 105,   7,   1,   5,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 125,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [148,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [194, 145,  32,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 180,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [137,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [194,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [193,  76,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [182,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 62,  51,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  37,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  61,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [162,  85,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [164,  86,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 77,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [193,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 136,  29,   3,   5,   6,   6,   6,   6],\n",
      "        [ 92,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  84,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 126,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 67,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [103, 174,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[ 17, 189,   0, 101, 111,   1,   5,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  69,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [140,   7,   7,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [180,  24,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 106,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  41,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [173,  70, 128,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  52, 171,  35,   1,   5,   6,   6,   6,   6],\n",
      "        [ 45,  36,  31,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 174,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 75,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 33,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 63, 122,  84,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 95,  11,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 36,  90,  53, 143,   1,   5,   6,   6,   6,   6],\n",
      "        [ 92,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [159,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 135,  79,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  72,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [164, 179,  89,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 74, 179,   7, 189,   0, 173,   3,   5,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [147,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [120,  82,  58,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  66, 152,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [138, 188,  14,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [113,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 136,  29,   3,   5,   6,   6,   6,   6],\n",
      "        [114,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [176, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [120,  82,  59,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 95, 165, 167,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [194,   7, 160,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [115,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 39,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [173,  70, 116,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [120,  26, 133,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 181,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 135,  79,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 97, 189,   0, 120,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [183,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [108,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 66,   7,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 19,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [185,  91,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[164,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [108,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [ 36, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 12, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 62,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 153,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  43, 115,   3,   5,   6,   6,   6,   6],\n",
      "        [193,  12,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 113, 165,   7,  54,   3,   5,   6,   6,   6],\n",
      "        [ 96,   7, 187,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [185,  30,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   0,   7,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [ 96, 165,  91,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [144,  70, 116,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 100,  65,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 36,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [120,  26, 192,  28,   3,   5,   6,   6,   6,   6],\n",
      "        [ 33,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [150, 189,   0, 101, 111,   3,   5,   6,   6,   6],\n",
      "        [155,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  53, 190,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [180,   2, 110,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [119,   7, 131,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [183,  43, 120,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 19,  91,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 158,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [178,   7, 133,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 13,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [176, 179,  64,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 109,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [194, 101,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [164,  85,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [180,  56,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 115, 144,  11,  84,   8,   5,   6,   6,   6],\n",
      "        [164,  38,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 190,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,  47,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [163,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   8,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,   9,   7, 174,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 109, 165,  29,   7,   3,   5,   6,   6,   6],\n",
      "        [ 66,   7,   1,  78,   1,   5,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 117,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [138, 120,  66,  14,   8,   5,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 94,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [101,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [105,  44,  10,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 20, 179, 134,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,  52,   7,  35,   1,   5,   6,   6,   6,   6],\n",
      "        [ 96, 165, 121,   3,   5,   6,   6,   6,   6,   6]])\n",
      "tensor([[  7,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [102, 174,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  73,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [180,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [162, 191,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [162, 179,  89,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [176, 179,  34,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [172,  32,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 36, 104,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [193, 180,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  41,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [162,  61,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [170, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [193,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 62,  70,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 68,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [178,  50,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  37,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 23, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  66,   7,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70, 126,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [140, 188,  14,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [160,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 104, 149,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [120,  26, 192,  58,   3,   5,   6,   6,   6,   6],\n",
      "        [ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [164,  37,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [120,  26,  84,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [140, 130,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [120,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 68,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,  76, 161,  53, 101,   7,   3,   5,   6,   6],\n",
      "        [ 36,  57,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [120,  26, 192,  59,   3,   5,   6,   6,   6,   6],\n",
      "        [ 32,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [173,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [173,   9, 133,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 23, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [124,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 100,  65,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 158,   7,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 173,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7, 145,   7,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [162,   7,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [162,  85,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 92,  70, 117,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 110,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 66,  25,   7,   1,   5,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165,  87,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [ 22, 190,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [102, 174,   1,   5,   6,   6,   6,   6,   6,   6]])\n",
      "tensor([[162,  37,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 99,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [ 75,  48,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 165, 121,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   7,   8,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [164,  38,   1,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [159,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 60,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [120,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [166,  66, 152,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [144,   7,  65,   8,   5,   6,   6,   6,   6,   6],\n",
      "        [  7, 189,   0, 173,   3,   5,   6,   6,   6,   6],\n",
      "        [180,  42, 173,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [173,   9,  84,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,  40,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   1,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [120,  26, 192,  27,   3,   5,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6],\n",
      "        [ 96, 181,  29,   3,   5,   6,   6,   6,   6,   6],\n",
      "        [127,   7,   3,   5,   6,   6,   6,   6,   6,   6],\n",
      "        [  7,   3,   5,   6,   6,   6,   6,   6,   6,   6]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "\tprint(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'idx_to_token',\n",
       " 'to_tokens',\n",
       " 'token_freqs',\n",
       " 'token_to_idx',\n",
       " 'unk']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'idx_to_token',\n",
       " 'to_tokens',\n",
       " 'token_freqs',\n",
       " 'token_to_idx',\n",
       " 'unk']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(src_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '!': 1,\n",
       " ',': 2,\n",
       " '.': 3,\n",
       " '<bos>': 4,\n",
       " '<eos>': 5,\n",
       " '<pad>': 6,\n",
       " '<unk>': 7,\n",
       " '?': 8,\n",
       " 'a': 9,\n",
       " 'aboient': 10,\n",
       " 'ai': 11,\n",
       " 'aide': 12,\n",
       " 'aide-moi': 13,\n",
       " 'aller': 14,\n",
       " 'allez': 15,\n",
       " 'allons-y': 16,\n",
       " 'appelle': 17,\n",
       " 'asseyez-vous': 18,\n",
       " 'assieds-toi': 19,\n",
       " 'attendez': 20,\n",
       " 'attends': 21,\n",
       " 'attrape': 22,\n",
       " 'attrapez': 23,\n",
       " 'au': 24,\n",
       " 'aucune': 25,\n",
       " 'avons': 26,\n",
       " 'battues': 27,\n",
       " 'battus': 28,\n",
       " 'bien': 29,\n",
       " 'bientt': 30,\n",
       " 'bizarre': 31,\n",
       " 'bon': 32,\n",
       " 'bonne': 33,\n",
       " 'boulot': 34,\n",
       " 'bras': 35,\n",
       " \"c'est\": 36,\n",
       " 'calme': 37,\n",
       " 'calmes': 38,\n",
       " 'calmez-vous': 39,\n",
       " 'ceci': 40,\n",
       " 'certain': 41,\n",
       " 'chercher': 42,\n",
       " 'chez': 43,\n",
       " 'chiens': 44,\n",
       " 'comme': 45,\n",
       " 'comment': 46,\n",
       " 'compris': 47,\n",
       " 'confiance': 48,\n",
       " 'continuez': 49,\n",
       " 'cours': 50,\n",
       " 'court': 51,\n",
       " 'dans': 52,\n",
       " 'de': 53,\n",
       " 'debout': 54,\n",
       " 'demande': 55,\n",
       " 'doucement': 56,\n",
       " 'du': 57,\n",
       " 'dfaites': 58,\n",
       " 'dfaits': 59,\n",
       " 'dgage': 60,\n",
       " 'dtendu': 61,\n",
       " 'elle': 62,\n",
       " 'elles': 63,\n",
       " 'emploi': 64,\n",
       " 'emport': 65,\n",
       " 'en': 66,\n",
       " 'entre': 67,\n",
       " 'entrez': 68,\n",
       " 'essaye': 69,\n",
       " 'est': 70,\n",
       " 'faible': 71,\n",
       " 'fainant': 72,\n",
       " 'fainante': 73,\n",
       " 'fais': 74,\n",
       " 'fais-moi': 75,\n",
       " 'fait': 76,\n",
       " 'fantastique': 77,\n",
       " 'feu': 78,\n",
       " 'fort': 79,\n",
       " 'foutre': 80,\n",
       " 'froid': 81,\n",
       " 'fmes': 82,\n",
       " 'gagnrent': 83,\n",
       " 'gagn': 84,\n",
       " 'gentil': 85,\n",
       " 'gentille': 86,\n",
       " 'gras': 87,\n",
       " 'gros': 88,\n",
       " 'homme': 89,\n",
       " 'hors': 90,\n",
       " 'ici': 91,\n",
       " 'il': 92,\n",
       " 'ils': 93,\n",
       " \"j'ai\": 94,\n",
       " \"j'en\": 95,\n",
       " 'je': 96,\n",
       " 'joignez-vous': 97,\n",
       " 'juste': 98,\n",
       " 'jai': 99,\n",
       " \"l'ai\": 100,\n",
       " 'la': 101,\n",
       " 'laisse': 102,\n",
       " 'laissez': 103,\n",
       " 'le': 104,\n",
       " 'les': 105,\n",
       " 'libre': 106,\n",
       " 'lit': 107,\n",
       " 'lche-toi': 108,\n",
       " \"m'en\": 109,\n",
       " 'maintenant': 110,\n",
       " 'maison': 111,\n",
       " 'malade': 112,\n",
       " 'me': 113,\n",
       " 'merci': 114,\n",
       " 'moi': 115,\n",
       " 'mort': 116,\n",
       " 'mouill': 117,\n",
       " 'mouvement': 118,\n",
       " 'ne': 119,\n",
       " 'nous': 120,\n",
       " 'occup': 121,\n",
       " 'ont': 122,\n",
       " 'oublie': 123,\n",
       " 'oublie-le': 124,\n",
       " 'paresseuse': 125,\n",
       " 'paresseux': 126,\n",
       " 'pars': 127,\n",
       " 'parti': 128,\n",
       " 'partie': 129,\n",
       " 'partir': 130,\n",
       " 'pas': 131,\n",
       " 'pay': 132,\n",
       " 'perdu': 133,\n",
       " 'peu': 134,\n",
       " 'plus': 135,\n",
       " 'porte': 136,\n",
       " 'poursuis': 137,\n",
       " 'pouvons-nous': 138,\n",
       " 'prie': 139,\n",
       " 'puis-je': 140,\n",
       " \"qu'est-ce\": 141,\n",
       " \"qu'on\": 142,\n",
       " 'question': 143,\n",
       " 'qui': 144,\n",
       " 'quoi': 145,\n",
       " 'recul': 146,\n",
       " 'reculez': 147,\n",
       " 'recule': 148,\n",
       " 'refuse': 149,\n",
       " 'rentre': 150,\n",
       " 'rentrez': 151,\n",
       " 'retard': 152,\n",
       " 'riche': 153,\n",
       " 'rouler': 154,\n",
       " 'rveille-toi': 155,\n",
       " 'rveillez-vous': 156,\n",
       " \"s'est\": 157,\n",
       " 'sais': 158,\n",
       " 'salut': 159,\n",
       " 'sant': 160,\n",
       " 'signe': 161,\n",
       " 'sois': 162,\n",
       " 'sors': 163,\n",
       " 'soyez': 164,\n",
       " 'suis': 165,\n",
       " 'suis-je': 166,\n",
       " 'sr': 167,\n",
       " 'tard': 168,\n",
       " 'te': 169,\n",
       " 'tenez': 170,\n",
       " 'tes': 171,\n",
       " 'tiens': 172,\n",
       " 'tom': 173,\n",
       " 'tomber': 174,\n",
       " 'triste': 175,\n",
       " 'trouve': 176,\n",
       " 'trouvez': 177,\n",
       " 'tu': 178,\n",
       " 'un': 179,\n",
       " 'va': 180,\n",
       " 'vais': 181,\n",
       " 'vas-y': 182,\n",
       " 'venez': 183,\n",
       " 'venu': 184,\n",
       " 'viens': 185,\n",
       " 'vous': 186,\n",
       " 'vu': 187,\n",
       " 'y': 188,\n",
       " '': 189,\n",
       " 'a': 190,\n",
       " 'quitable': 191,\n",
       " 't': 192,\n",
       " 'a': 193,\n",
       " '': 194}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " '<bos>': 3,\n",
       " '<eos>': 4,\n",
       " '<pad>': 5,\n",
       " '<unk>': 6,\n",
       " '?': 7,\n",
       " 'a': 8,\n",
       " 'agree': 9,\n",
       " 'ahead': 10,\n",
       " 'am': 11,\n",
       " 'ask': 12,\n",
       " 'attack': 13,\n",
       " 'away': 14,\n",
       " 'back': 15,\n",
       " 'bark': 16,\n",
       " 'be': 17,\n",
       " 'beats': 18,\n",
       " 'bed': 19,\n",
       " 'beg': 20,\n",
       " 'busy': 21,\n",
       " 'call': 22,\n",
       " 'calm': 23,\n",
       " 'came': 24,\n",
       " 'can': 25,\n",
       " 'catch': 26,\n",
       " 'cheers': 27,\n",
       " 'cold': 28,\n",
       " 'come': 29,\n",
       " 'cool': 30,\n",
       " 'cringed': 31,\n",
       " 'deaf': 32,\n",
       " 'did': 33,\n",
       " 'die': 34,\n",
       " 'died': 35,\n",
       " 'dogs': 36,\n",
       " \"don't\": 37,\n",
       " 'down': 38,\n",
       " 'dozed': 39,\n",
       " 'drive': 40,\n",
       " 'drop': 41,\n",
       " 'excuse': 42,\n",
       " 'fair': 43,\n",
       " 'fat': 44,\n",
       " 'feel': 45,\n",
       " 'fell': 46,\n",
       " 'find': 47,\n",
       " 'fine': 48,\n",
       " 'fire': 49,\n",
       " 'fix': 50,\n",
       " 'follow': 51,\n",
       " 'for': 52,\n",
       " 'forget': 53,\n",
       " 'free': 54,\n",
       " 'full': 55,\n",
       " 'fun': 56,\n",
       " 'game': 57,\n",
       " 'get': 58,\n",
       " 'go': 59,\n",
       " 'good': 60,\n",
       " 'got': 61,\n",
       " 'grab': 62,\n",
       " 'hang': 63,\n",
       " 'have': 64,\n",
       " 'he': 65,\n",
       " \"he's\": 66,\n",
       " 'hello': 67,\n",
       " 'help': 68,\n",
       " 'here': 69,\n",
       " 'hi': 70,\n",
       " 'him': 71,\n",
       " 'his': 72,\n",
       " 'hit': 73,\n",
       " 'hold': 74,\n",
       " 'home': 75,\n",
       " 'hop': 76,\n",
       " 'how': 77,\n",
       " \"how's\": 78,\n",
       " 'hug': 79,\n",
       " 'hurry': 80,\n",
       " 'i': 81,\n",
       " \"i'll\": 82,\n",
       " \"i'm\": 83,\n",
       " \"i've\": 84,\n",
       " 'ill': 85,\n",
       " 'in': 86,\n",
       " 'is': 87,\n",
       " 'it': 88,\n",
       " \"it's\": 89,\n",
       " 'job': 90,\n",
       " 'join': 91,\n",
       " 'keep': 92,\n",
       " 'kiss': 93,\n",
       " 'know': 94,\n",
       " 'late': 95,\n",
       " 'lazy': 96,\n",
       " 'leave': 97,\n",
       " 'left': 98,\n",
       " \"let's\": 99,\n",
       " 'look': 100,\n",
       " 'lost': 101,\n",
       " 'luck': 102,\n",
       " 'man': 103,\n",
       " 'marry': 104,\n",
       " 'may': 105,\n",
       " 'me': 106,\n",
       " 'new': 107,\n",
       " 'nice': 108,\n",
       " 'no': 109,\n",
       " 'now': 110,\n",
       " 'off': 111,\n",
       " 'ok': 112,\n",
       " 'okay': 113,\n",
       " 'on': 114,\n",
       " 'open': 115,\n",
       " 'out': 116,\n",
       " 'over': 117,\n",
       " 'paid': 118,\n",
       " 'phoned': 119,\n",
       " 'quit': 120,\n",
       " 'ready': 121,\n",
       " 'really': 122,\n",
       " 'refuse': 123,\n",
       " 'rested': 124,\n",
       " 'rich': 125,\n",
       " 'run': 126,\n",
       " 'runs': 127,\n",
       " 'sad': 128,\n",
       " 'save': 129,\n",
       " 'saw': 130,\n",
       " 'seated': 131,\n",
       " 'see': 132,\n",
       " 'she': 133,\n",
       " 'show': 134,\n",
       " 'shut': 135,\n",
       " 'sick': 136,\n",
       " 'sit': 137,\n",
       " 'slow': 138,\n",
       " 'some': 139,\n",
       " 'soon': 140,\n",
       " 'speak': 141,\n",
       " 'stayed': 142,\n",
       " 'still': 143,\n",
       " 'stood': 144,\n",
       " 'stop': 145,\n",
       " 'sure': 146,\n",
       " 'swore': 147,\n",
       " 'take': 148,\n",
       " 'taste': 149,\n",
       " 'tell': 150,\n",
       " 'terrific': 151,\n",
       " 'that': 152,\n",
       " 'them': 153,\n",
       " 'they': 154,\n",
       " 'this': 155,\n",
       " 'tidy': 156,\n",
       " 'to': 157,\n",
       " 'tom': 158,\n",
       " 'too': 159,\n",
       " 'tried': 160,\n",
       " 'trust': 161,\n",
       " 'try': 162,\n",
       " 'ugly': 163,\n",
       " 'up': 164,\n",
       " 'us': 165,\n",
       " 'use': 166,\n",
       " 'wait': 167,\n",
       " 'wake': 168,\n",
       " 'warn': 169,\n",
       " 'wash': 170,\n",
       " 'watch': 171,\n",
       " 'way': 172,\n",
       " 'we': 173,\n",
       " 'weak': 174,\n",
       " 'well': 175,\n",
       " 'wet': 176,\n",
       " 'what': 177,\n",
       " 'who': 178,\n",
       " 'win': 179,\n",
       " 'won': 180,\n",
       " 'works': 181,\n",
       " 'write': 182,\n",
       " 'you': 183}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "184\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))\n",
    "print(len(src_vocab))\n",
    "print(len(tgt_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.030, 3493.6 tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"180.65625pt\" viewBox=\"0 0 262.1875 180.65625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-07-08T11:13:33.260146</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 262.1875 180.65625 \nL 262.1875 0 \nL 0 0 \nL 0 180.65625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 91.259539 143.1 \nL 91.259539 7.2 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m44e1ce5418\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m44e1ce5418\" x=\"91.259539\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 50 -->\n      <g transform=\"translate(84.897039 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 142.654276 143.1 \nL 142.654276 7.2 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m44e1ce5418\" x=\"142.654276\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(133.110526 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 194.049013 143.1 \nL 194.049013 7.2 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m44e1ce5418\" x=\"194.049013\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 150 -->\n      <g transform=\"translate(184.505263 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m44e1ce5418\" x=\"245.44375\" y=\"143.1\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 200 -->\n      <g transform=\"translate(235.9 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- epoch -->\n     <g transform=\"translate(132.565625 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 50.14375 119.964255 \nL 245.44375 119.964255 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m0e045109e5\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m0e045109e5\" x=\"50.14375\" y=\"119.964255\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 123.763474)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 50.14375 77.831695 \nL 245.44375 77.831695 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m0e045109e5\" x=\"50.14375\" y=\"77.831695\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 81.630913)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 50.14375 35.699134 \nL 245.44375 35.699134 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m0e045109e5\" x=\"50.14375\" y=\"35.699134\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 39.498353)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- loss -->\n     <g transform=\"translate(14.798437 84.807812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 50.14375 13.377273 \nL 60.422697 57.917139 \nL 70.701645 82.085948 \nL 80.980592 99.231281 \nL 91.259539 108.541237 \nL 101.538487 117.442621 \nL 111.817434 119.689727 \nL 122.096382 122.727021 \nL 132.375329 124.561029 \nL 142.654276 129.304966 \nL 152.933224 128.955836 \nL 163.212171 132.189989 \nL 173.491118 132.322273 \nL 183.770066 133.083317 \nL 194.049013 132.667679 \nL 204.327961 133.107477 \nL 214.606908 133.233759 \nL 224.885855 135.794356 \nL 235.164803 135.08255 \nL 245.44375 136.922727 \n\" clip-path=\"url(#pd0eb06f460)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 143.1 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd0eb06f460\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"135.9\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(\n",
    "\tlen(src_vocab), key_size, query_size, value_size, num_hiddens, \n",
    "\tnorm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "\tnum_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "\tlen(tgt_vocab), key_size, query_size, value_size, num_hiddens, \n",
    "\tnorm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "\tnum_layers, dropout)\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "go . => va !,  bleu 1.000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i lost . => j'ai perdu .,  bleu 1.000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "he's calm . => il est mouill .,  bleu 0.658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "i'm home . => je suis chez moi .,  bleu 1.000\n"
     ]
    }
   ],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    print(f'{eng} => {translation}, ', f'bleu {d2l.bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generative ability test on protein dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters of model\n",
    "\n",
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 1, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, d2l.try_gpu()\n",
    "ffn_num_input, ffn_num_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32] # 32 corresponds to the dim of such number to normalize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspection of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\10331\\\\Haowen_academic_in_use\\\\UROP\\\\Transformer_based_model'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING mybio.py module not found - excecution may fail\n",
      "WARNING zyggregator not found!!!\n"
     ]
    }
   ],
   "source": [
    "data_list_antiparallel= []\n",
    "data_list_parallel= []\n",
    "\n",
    "for i in range(1, 9):\n",
    "\twith open('BSn_libraries/BSn_libraries_copy/anti_frag_dic_{}.pkl'.format(i), 'rb') as f:\n",
    "\t\tdata = pickle.load(f, encoding='latin1')\n",
    "\t\tdata_list_antiparallel.append(data)\n",
    "\n",
    "\twith open('BSn_libraries/BSn_libraries_copy/para_frag_dic_{}.pkl'.format(i), 'rb') as f:\n",
    "\t\tdata = pickle.load(f, encoding='latin1')\n",
    "\t\tdata_list_parallel.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working score: $C = \\sum_{i=1}^{n} (l_i^2c_i - 0.01l_ip_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 for parallel\n",
    "# 1 for antiparallel\n",
    "\n",
    "# target, complementary_seq, counts, promiscuity, length, working_score, hb_pattern, para/anti\n",
    "BSn_data = []\n",
    "least_length = 3\n",
    "\n",
    "for frag_i_data in data_list_parallel[least_length-1:]:\n",
    "\tfor keys in frag_i_data.keys():\n",
    "\n",
    "\t\tlength = len(keys)\n",
    "\t\tfor element in frag_i_data[keys]:\n",
    "\n",
    "\t\t\tworking_score = length**2 * element.count_score - 0.01 * length * element.promiscuity_score\n",
    "\t\t\tlist_i = [keys, element.complementary_sequence, element.count_score, element.promiscuity_score, length, working_score, element.hb_pattern, 0]\n",
    "\t\t\tBSn_data.append(list_i)\n",
    "\n",
    "for frag_i_data in data_list_antiparallel[least_length-1:]:\n",
    "\tfor keys in frag_i_data.keys():\n",
    "\t\tlength = len(keys)\n",
    "\t\tfor element in frag_i_data[keys]:\n",
    "\n",
    "\t\t\tworking_score = length**2 * element.count_score - 0.01 * length * element.promiscuity_score\n",
    "\t\t\tlist_i = [keys, element.complementary_sequence, element.count_score, element.promiscuity_score, length, working_score, element.hb_pattern, 1]\n",
    "\t\t\tBSn_data.append(list_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2206533"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(BSn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VAA', 'IVV', 1, 606, 3, -9.18, ((0, 0), (None, None), (0, 0)), 1]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSn_data[500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tailor the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_list1=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_dict = {\n",
    "\t\t'<bos>': 1, \n",
    "\t\t'<eos>': 2, \n",
    "\t\t'<pad>': 3, \n",
    "\t\t'<unk>': 4,\n",
    "\t\t'A': 5,\n",
    "\t\t'C': 6,\n",
    "\t\t'D': 7, \n",
    "\t\t'E': 8,\n",
    "\t\t'F': 9, \n",
    "\t\t'G': 10, \n",
    "\t\t'H': 11,\n",
    "\t\t'I': 12, \n",
    "\t\t'K': 13, \n",
    "\t\t'L': 14, \n",
    "\t\t'M': 15, \n",
    "\t\t'N': 16, \n",
    "\t\t'P': 17, \n",
    "\t\t'Q': 18, \n",
    "\t\t'R': 19, \n",
    "\t\t'S': 20, \n",
    "\t\t'T': 21, \n",
    "\t\t'V': 22, \n",
    "\t\t'W': 23, \n",
    "\t\t'Y': 24 \n",
    "\t\t }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10331\\AppData\\Local\\Temp\\ipykernel_12864\\910978762.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  BSn_data_dataset1 = np.array(BSn_data)[:, 0:2]\n"
     ]
    }
   ],
   "source": [
    "BSn_data_dataset1 = np.array(BSn_data)[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['SPY', 'WHA'],\n",
       "       ['SPY', 'LVL'],\n",
       "       ['SPQ', 'GFD'],\n",
       "       ...,\n",
       "       ['SFILKSFR', 'EAQVWIAM'],\n",
       "       ['LDLRNFYQ', 'LTYNVIFR'],\n",
       "       ['LDLRNFYQ', 'LVFGQSWN']], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSn_data_dataset1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78bf9874701a3957232b8c741c196d0ac3fa790968bd4270c0e2fd5bdf999274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
