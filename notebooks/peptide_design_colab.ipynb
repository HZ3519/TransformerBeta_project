{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBeta: An Introduction\n",
    "\n",
    "TransformerBeta is a generative model based on Transformer architecture, developed to generate complementary binder for linear peptide epitopes of length 8 in an antiparallel beta strand conformation. The model is trained on a curated dataset of length 8 antiparallel beta strand pairs from the AF2 Beta Strand Database.\n",
    "\n",
    "![TransformerBeta.png](data:image/png;base64,iVBO)\n",
    "\n",
    "This Google Colaboratory notebook serves as an accessible user interface for TransformerBeta. It allows users to effortlessly generate, predict, and evaluate length 8 antiparallel beta interactions using the TransformerBeta model. The current notebook uses the model's best performing version, M retrain model, which was retrained on the complete training set as indicated in the research paper.\n",
    "\n",
    "For detailed insight into TransformerBeta, we suggest referring to our research paper (yet to be released). \n",
    "\n",
    "To install TransformerBeta locally for your projects, visit: [TransformerBeta on Github](https://github.com/HZ3519/TransformerBeta). \n",
    "\n",
    "For individual usage of the AF2 Beta Strand Database, check out: [AF2 Beta Strand Database on Huggingface](https://huggingface.co/datasets/hz3519/AF2_Beta_Strand_Database/tree/main). \n",
    "\n",
    "For accessing the model weights and corresponding training, validation, and test sets mentioned in the paper, refer to: [TransformerBeta on Huggingface](https://huggingface.co/hz3519/TransformerBeta).\n",
    "\n",
    "## Outline of the Notebook\n",
    "1. Section 1: Setup Guidance\n",
    "2. Section 2: Model Loading\n",
    "3. Section 3: Generation of Peptide Sequences (4 different methods: 1. Iterative sampling of N unique peptides given a target sequence, 2. Random sampling of a peptide given a target sequence, 3. Greedy prediction of a peptide given a target sequence, 4. Evaluation of beta conformation probability given both the target and the peptide sequence)\n",
    "4. Section 4: Analysis of Iterative Sampling Results (if you choose for this method in Section 3)\n",
    "\n",
    "## Notebook Usage Instructions\n",
    "Please following the instructions for each section to run the notebook successfully.\n",
    "\n",
    "## Licensing of TransformerBeta\n",
    "\n",
    "TransformerBeta, inclusive of the model weights, training set, validation set, and test set, along with the AF2 Beta Strand Database, is distributed under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n",
    "\n",
    "**Kindly ensure that you adhere to the conditions stipulated by these licenses when using these files.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup Guidance\n",
    "\n",
    "To ensure successful execution of the notebook, please follow the instructions provided in each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setting up Google Drive Connection\n",
    "#@markdown To run TransformerBeta, a connection with your Google Drive is essential. This allows the program to save and access the necessary files.\n",
    "\n",
    "#@markdown **Step 1**: Execute this cell by pressing `Ctrl+Enter` or by clicking the **Play** button to the left.\n",
    "\n",
    "# This chunk will mount Google Drive to allow the storage and retrieval of files\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "drive.mount('/content/gdrive') # This path is where all output will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installing Dependencies and GitHub TransfromerBeta Repository\n",
    "#@markdown Ensure your runtime environment is set to CPU. \n",
    "\n",
    "#@markdown **Step 2**: Navigate to `Runtime --> Change runtime type` in the menu above and set the Hardware Accelerator to 'None'.\n",
    "\n",
    "#@markdown **Step 3**: Execute this cell to install the necessary dependencies by pressing `Ctrl+Enter` or by clicking the **Play** button to the left. \n",
    "\n",
    "# This chunk will clone and install the TransformerBeta repository from GitHub and the necessary dependencies\n",
    "import shutil\n",
    "try:\n",
    "  shutil.rmtree('/content/TransformerBeta', ignore_errors=True)\n",
    "except:\n",
    "  print('')\n",
    "\n",
    "# personal token for testing repo: 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\n",
    "# if github repo released public\n",
    "# !git clone https://github.com/HZ3519/TransformerBeta_project.git\n",
    "# personal token for testing repo: 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\n",
    "!git clone https://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@github.com/HZ3519/TransformerBeta_project.git\n",
    "!pip install -r ./TransformerBeta_project/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Restarting Runtime\n",
    "#@markdown Once the installation of the necessary packages is completed, a restart of the runtime is required for the changes to take effect.\n",
    "#@markdown **Step 4**: Navigate to `Runtime --> Restart runtime` in the menu above to restart.\n",
    "\n",
    "#@markdown After-restart, the connection to Google Drive needs to be reestablished.\n",
    "#@markdown **Step 5**: Execute this cell again by pressing `Ctrl+Enter` or by clicking the **Play** button to the left to reconnect Google Drive.\n",
    "\n",
    "# reconnect to Google drive\n",
    "!pip install ./TransformerBeta_project\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown The default model is \"model M retrain\". **Step 1**: Execute this cell by pressing `Ctrl+Enter` or by clicking the **Play** button to the left.\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from TransformerBeta import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_name = \"model_M_retrain\" #@param {type:\"string\"}\n",
    "\n",
    "# git clone the model directory from huggerface\n",
    "!git lfs install\n",
    "# if the hugging face repo is public, change it:\n",
    "# !git clone https://huggingface.co/hz3519/TransformerBeta_models\n",
    "# personal token for testing repo: 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\n",
    "!git clone https://hz3519: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@huggingface.co/hz3519/TransformerBeta_models\n",
    "\n",
    "model_dir = \"TransformerBeta_models/{}\".format(model_name)\n",
    "\n",
    "# Load the config\n",
    "with open(\"{}/config.json\".format(model_dir), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create instances of your encoder and decoder\n",
    "encoder_standard = TransformerEncoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"])\n",
    "decoder_standard = TransformerDecoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"], shared_embedding=encoder_standard.embedding)\n",
    "\n",
    "# Create an instance of your model\n",
    "model_standard = EncoderDecoder(encoder_standard, decoder_standard)\n",
    "model_standard_total_params = sum(p.numel() for p in model_standard.parameters())\n",
    "model_standard_total_trainable_params = sum(p.numel() for p in model_standard.parameters() if p.requires_grad)\n",
    "\n",
    "# Load the model's state_dict\n",
    "state_dict = torch.load(\"{}/model_weights.pth\".format(model_dir), map_location='cpu')\n",
    "\n",
    "# If the state_dict was saved with 'module' prefix due to DataParallel\n",
    "# Remove 'module' prefix if present\n",
    "if list(state_dict.keys())[0].startswith('module'):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "model_standard.load_state_dict(state_dict)\n",
    "\n",
    "model_use = model_standard \n",
    "prediction_length = 8 \n",
    "\n",
    "# create a txt file to record the results\n",
    "if not os.path.exists('model_prediction'):\n",
    "\tos.mkdir('model_prediction')\n",
    "\n",
    "print('Transformer model loaded: total number of parameters: {}'.format(model_standard_total_params))\n",
    "print('Transformer model loaded:: total number of trainable parameters: {}'.format(model_standard_total_trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title #Follow all steps outlined below to design a binder.\n",
    "#@markdown To try the **test case** [1ssc](https://www.rcsb.org/3d-view/1SSC), press the play button to the left.\n",
    "\\\n",
    "#@markdown If you don't want to run the test case, **change the input parameters**.\n",
    "\n",
    "#@markdown #Parameters\n",
    "#@markdown - *PDBID* - PDB id of the receptor structure \n",
    "#@markdown - *RECEPTOR_CHAIN* - what chain in the PDB file to use as receptor\n",
    "#@markdown - **Optional**: *UPLOAD_PDB* - if you prefer to upload a file instead, you can simply do this. See \"Upload the MSA\" below and ensure the PDBID matches the name of your uploaded file.\n",
    "#@markdown - *RECPTOR_SEQUENCE* - amino acid sequence of the receptor protein chain\n",
    "#@markdown - *TARGET_RESIDUES* - what residue numbers in the receptor sequence to design a binder towards. Separated by commas.\n",
    "#@markdown - *BINDER_LENGTH* \n",
    "#@markdown - **Optional**: *START_SEQUENCE* - sequence to start the optimisation from. If no sequence is provided, a random sequence is initialised. If you don't have a good start sequence, leave this empty. If you want to simply predict a protein-peptide interaction, input a start sequence and set NITER to 1, then download the result below.\n",
    "#@markdown - *NITER: Number of iterations* - how many iterations to optimise (default=300) \n",
    "#@markdown - *Binder centre of mass (COM) using alpha carbons (relative to the receptor structure)*\n",
    "#@markdown - **Optional**: *Calculate Binder COM based on the target residues.* Make sure this is an appropriate COM for you, otherwise change it manually.\n",
    "#@markdown - **Receptor MSA** - currently no MSA search is available directly in this notebook, therefore you have to provide your own MSA in a3m format and upload it here. \\\n",
    "#@markdown There are two ways of doing this: \\\n",
    "#@markdown 1. Search uniclust_30 locally with HHblits \\\n",
    "#@markdown 2. Go to https://toolkit.tuebingen.mpg.de/tools/hhblits \\\n",
    "#@markdown Paste the receptor sequence in the search field in fasta format --> Submit. \\\n",
    "#@markdown When the search is finished, go to the tab \"Query Template MSA\" and \"Download Full A3M\" \\ \n",
    "#@markdown - Upload the MSA here: \\\n",
    "#@markdown Click the folder icon (Files) to the left and select the upload file icon. Upload the .a3m file.\n",
    "#@markdown Make sure the MSA is named **PDBID**_receptor.a3m, where PDBID is the PDBID specified above.\n",
    "import sys, os\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import py3Dmol\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "sys.path.insert(0,'/content/EvoBind/src')\n",
    "PDBID = \"1ssc\" #@param {type:\"string\"}\n",
    "RECEPTOR_CHAIN = \"A\" #@param {type:\"string\"}\n",
    "UPLOAD_PDB = False #@param {type:\"boolean\"}\n",
    "RECEPTOR_SEQUENCE = \"KETAAAKFERQHMDSSTSAASSSNYCNQMMKSRNLTKDRCKPVNTFVHESLADVQAVCSQKNVACKNGQTNCYQSYSTMSITDCRETGSSKYPNCAYKTTQANKHIIVACEG\" #@param {type:\"string\"}\n",
    "TARGET_RESIDUES = \"4,5,8,11,12,45,47,54,55,57,58,59,65,66,72,74,81,83,102,104,105,106,107,108,109,110,111,112\" #@param {type:\"string\"}\n",
    "TARGET_RESIDUES = [int(x) for x in TARGET_RESIDUES.split(',')]\n",
    "CALCULATE_BINDER_COM = False #@param {type:\"boolean\"}\n",
    "BINDER_LENGTH =  11#@param {type:\"integer\"}\n",
    "START_SEQUENCE = \"DIQMVNMQVAE\" #@param {type:\"string\"}\n",
    "NITER =  300#@param {type:\"integer\"}\n",
    "BINDER_COM = \"33.966637,23.854908,9.859454\" #@param {type:\"string\"}\n",
    "BINDER_COM = [float(x) for x in BINDER_COM.split(',')]\n",
    "RECEPTOR_MSA = \"1ssc_receptor.a3m\" #@param {type:\"string\"}\n",
    "OUTDIR=\"/content/gdrive/MyDrive/\"+PDBID+'/'\n",
    "#Make outdir\n",
    "if not os.path.exists(OUTDIR):\n",
    "  os.mkdir(OUTDIR)\n",
    "#Get structure\n",
    "RECEPTOR_STRUCTURE = \"https://files.rcsb.org/download/\"+PDBID\n",
    "\n",
    "#Try .pdb\n",
    "if UPLOAD_PDB==True:\n",
    "  RECEPTOR_STRUCTURE='/content/'+PDBID+'.pdb'\n",
    "else:\n",
    "  if not os.path.exists(OUTDIR+PDBID+\".pdb\"):\n",
    "    try:\n",
    "      urllib.request.urlretrieve(RECEPTOR_STRUCTURE+\".pdb\", OUTDIR+PDBID+\".pdb\")\n",
    "    except:\n",
    "      print('')\n",
    "  #Otherwise .cif\n",
    "  if not os.path.exists(OUTDIR+PDBID+\".pdb\") and not os.path.exists(OUTDIR+PDBID+\".cif\"):\n",
    "    try:\n",
    "      urllib.request.urlretrieve(RECEPTOR_STRUCTURE+\".cif\", OUTDIR+PDBID+\".cif\")\n",
    "    except:\n",
    "      print(\"Can't download file: \"+RECEPTOR_STRUCTURE+'. Ensure that the PDBID is correct.')\n",
    "\n",
    "  if len(glob.glob(OUTDIR+PDBID+\".pdb\"))>0:\n",
    "    RECEPTOR_STRUCTURE=OUTDIR+PDBID+\".pdb\"\n",
    "  else:\n",
    "    RECEPTOR_STRUCTURE=OUTDIR+PDBID+\".cif\"\n",
    "  \n",
    "#Read and display the PDB file\n",
    "sys.path.insert(0,'/content/EvoBind/src/')\n",
    "from prepare_input_colab import prepare_input\n",
    "#Get the receptor CAs\n",
    "RECEPTOR_CAs, RECEPTOR_PDBSEQ = prepare_input(RECEPTOR_STRUCTURE, RECEPTOR_CHAIN, TARGET_RESIDUES, BINDER_COM, OUTDIR)\n",
    "#Check if CALCULATE_BINDER_COM\n",
    "if CALCULATE_BINDER_COM==True:\n",
    "  TARGET_CAs = RECEPTOR_CAs[np.array(TARGET_RESIDUES)-1]\n",
    "  BINDER_COM = np.sum(TARGET_CAs,axis=0)/(TARGET_CAs.shape[0])\n",
    "  RECEPTOR_CAs, RECEPTOR_PDBSEQ = prepare_input(RECEPTOR_STRUCTURE, RECEPTOR_CHAIN, TARGET_RESIDUES, BINDER_COM, OUTDIR)\n",
    "  print('The calculated binder COM is:', BINDER_COM)\n",
    "\n",
    "#Print the receptor PDB sequence\n",
    "print('The receptor sequence according to the specified PDB file is:',RECEPTOR_PDBSEQ)\n",
    "print('Make sure this is the sequence you are using for the MSA as well.')\n",
    "#Check the sequence and structure\n",
    "if RECEPTOR_SEQUENCE!=RECEPTOR_PDBSEQ:\n",
    "  print('ERROR!')\n",
    "  print('The specified sequence and the sequence in the PDB file are not identical.')\n",
    "  print('Specified sequence:', RECEPTOR_SEQUENCE)\n",
    "  print('PDB sequence', RECEPTOR_PDBSEQ)\n",
    "else:\n",
    "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
    "  view.addModel(open(OUTDIR+\"receptor.pdb\",'r').read(),'pdb')\n",
    "  view.setStyle({'chain':'A'},{'cartoon': {'color':'green'}})\n",
    "  view.addModel(open(OUTDIR+\"receptor_target_residues.pdb\",'r').read(),'pdb')\n",
    "  view.setStyle({'chain':'B'},{'stick':{}})\n",
    "  view.addModel(open(OUTDIR+\"COM.pdb\",'r').read(),'pdb')\n",
    "  view.setStyle({'chain':'C'},{'sphere': {'color':'cyan'}})\n",
    "  view.zoomTo()\n",
    "  view.show()\n",
    "\n",
    "#Check the MSA\n",
    "if PDBID=='1ssc':\n",
    "  RECEPTOR_MSA = '/content/EvoBind/data/test/'+RECEPTOR_MSA\n",
    "else:\n",
    "  RECEPTOR_MSA ='/content/'+RECEPTOR_MSA\n",
    "if not os.path.exists(RECEPTOR_MSA):\n",
    "  print(\"Can't find MSA:\",RECEPTOR_MSA) \n",
    "else:\n",
    "  print('Using MSA:',RECEPTOR_MSA)\n",
    "\n",
    "from check_msa_colab import process_a3m\n",
    "PROCESSED_MSA=RECEPTOR_MSA.split('.')[0]+'_processed.a3m'\n",
    "process_a3m(RECEPTOR_MSA, RECEPTOR_SEQUENCE, PROCESSED_MSA)\n",
    "RECEPTOR_MSA=PROCESSED_MSA\n",
    "#Write fasta\n",
    "RECEPTOR_FASTA=OUTDIR+PDBID+'_receptor.fasta'\n",
    "with open(RECEPTOR_FASTA, 'w') as file:\n",
    "  file.write('>'+PDBID+'\\n')\n",
    "  file.write(RECEPTOR_SEQUENCE)\n",
    "\n",
    "#Check the start sequence length\n",
    "if len(START_SEQUENCE)>0:\n",
    "  if len(START_SEQUENCE)!=BINDER_LENGTH:\n",
    "    print('The starting sequence length does not match the binder length')\n",
    "    \n",
    "#@markdown ----\n",
    "#@markdown \\\n",
    "#@markdown ### The receptor is depicted in green in cartoon format, the target residues in stick format and the binder centre of mass in cyan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown #Run *EvoBind*\n",
    "\n",
    "#@markdown Click play to design a binder. \n",
    "\n",
    "#@markdown The whole process will take approximately **7 hours** (for 300 iterations). Relax and wait for your binder. \n",
    "#@markdown The run will continue where you left it if it was interrupted for some reason.\n",
    "\n",
    "#@markdown The iteration, interface distance for the peptide, interface distance for the receptor, plDDT, delta COM, loss and best peptide sequence are displayed after each iteration.\n",
    "\n",
    "#@markdown The AF2 params are fetched here (if they are not already downloaded).\n",
    "import shutil\n",
    "PARAMS=\"/content/gdrive/MyDrive/AF/params/\"\n",
    "if not os.path.exists(PARAMS):\n",
    "  if not os.path.exists('/content/gdrive/MyDrive/AF/'):\n",
    "    os.mkdir('/content/gdrive/MyDrive/AF/')\n",
    "  os.mkdir(PARAMS)\n",
    "  !wget https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar \n",
    "  shutil.move('/content/alphafold_params_2021-07-14.tar', PARAMS)\n",
    "  #Extract\n",
    "  !tar -xvf /content/gdrive/MyDrive/AF/params/alphafold_params_2021-07-14.tar -C /content/gdrive/MyDrive/AF/params/\n",
    "sys.path.insert(0,'/content/EvoBind/src/AF2')\n",
    "\n",
    "\n",
    "from mc_design_colab import main\n",
    "MAX_RECYCLES=8 #max_recycles (default=3)\n",
    "MODEL_NAME='model_1' #model_1_ptm\n",
    "main(RECEPTOR_FASTA, 'design', TARGET_RESIDUES, RECEPTOR_CAs,\n",
    "     RECEPTOR_MSA, BINDER_LENGTH, BINDER_COM, OUTDIR, NITER,\n",
    "     [MODEL_NAME], MAX_RECYCLES, \"/content/gdrive/MyDrive/AF/\", START_SEQUENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown #Analyse the results\n",
    "#@markdown The TOP_FRACTION represents how many percent of the designs to select. \n",
    "\n",
    "#@markdown Only the best model is visualised. As a rule of thumb, a **plDDT value above 80** represents a reliable binder.\n",
    "\n",
    "#@markdown Click the DOWNLOAD box to download the top models and their sequences.\n",
    "\n",
    "#@markdown Click the DOWNLOAD_START box to download the start model.\n",
    "\n",
    "TOP_FRACTION =  100#@param {type:\"integer\"}\n",
    "RECEPTOR_STYLE = \"stick\" #@param [\"cartoon\", \"sphere\", \"stick\"]\n",
    "BINDER_STYLE = \"stick\" #@param [\"cartoon\", \"sphere\", \"stick\"]\n",
    "DOWNLOAD = False #@param {type:\"boolean\"}\n",
    "DOWNLOAD_START = False #@param {type:\"boolean\"}\n",
    "loss = np.load(OUTDIR+'loss.npy')[1:]\n",
    "seqs = np.load(OUTDIR+'sequence.npy')[1:]\n",
    "plddt = np.load(OUTDIR+'plddt.npy')[1:]\n",
    "#Get top\n",
    "sorted_models = np.argsort(loss)\n",
    "n_select = int(TOP_FRACTION/100*len(loss))\n",
    "top_loss = loss[sorted_models][:n_select]\n",
    "top_sequence = seqs[sorted_models][:n_select]\n",
    "top_plddt = plddt[sorted_models][:n_select]\n",
    "top_models = sorted_models[:n_select]\n",
    "\n",
    "#Print\n",
    "print('The best sequences, losses and plDDT values are:')\n",
    "for i in range(len(top_loss)):\n",
    "  print(top_sequence[i], top_loss[i], top_plddt[i])\n",
    "#Vis\n",
    "view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',) \n",
    "view.addModel(open(OUTDIR+'unrelaxed_'+str(top_models[0])+'.pdb','r').read(),'pdb')\n",
    "view.setStyle({'chain':'A'},{RECEPTOR_STYLE: {'color':'green'}})\n",
    "view.setStyle({'chain':'B'},{BINDER_STYLE: {'color':'cyan'}})\n",
    "view.zoomTo()\n",
    "view.show()\n",
    "\n",
    "#@title Download the results\n",
    "import shutil\n",
    "if not os.path.exists(OUTDIR+'best_models'):\n",
    "  os.mkdir(OUTDIR+'best_models')\n",
    "\n",
    "#Download\n",
    "if DOWNLOAD==True:\n",
    "  rank=1\n",
    "  for model in top_models:\n",
    "    shutil.copy(OUTDIR+'unrelaxed_'+str(model)+'.pdb', OUTDIR+'best_models/rank_'+str(rank)+'.pdb')\n",
    "    rank+=1\n",
    "\n",
    "  for file in glob.glob(OUTDIR+'best_models/rank_*.pdb'):\n",
    "    files.download(file)\n",
    "\n",
    "  #Write a fasta file with the top seqs\n",
    "  rank=1\n",
    "  with open(OUTDIR+'best_models/top_seqs.fasta', 'w') as file:\n",
    "    for seq in top_sequence:\n",
    "      file.write('>rank_'+str(rank)+'\\n')\n",
    "      file.write(seq+'\\n')\n",
    "      rank+=1\n",
    "  files.download(OUTDIR+'best_models/top_seqs.fasta')\n",
    "\n",
    "if DOWNLOAD_START==True:\n",
    "  files.download(OUTDIR+'unrelaxed_start.pdb')\n",
    "#@markdown ### The receptor is depicted in green and the binder in cyan. Change the style above to view the design differently. \n",
    "#@markdown ### Try the sphere representation to see how all atoms fit together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model directory (please specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model_M_retrain\" # model directory for loading the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model weights (run & do not modify the codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change the following code of loading the model\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from TransformerBeta import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load the config\n",
    "with open(\"{}/config.json\".format(model_dir), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create instances of your encoder and decoder\n",
    "encoder_standard = TransformerEncoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"])\n",
    "decoder_standard = TransformerDecoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"], shared_embedding=encoder_standard.embedding)\n",
    "\n",
    "# Create an instance of your model\n",
    "model_standard = EncoderDecoder(encoder_standard, decoder_standard)\n",
    "model_standard_total_params = sum(p.numel() for p in model_standard.parameters())\n",
    "model_standard_total_trainable_params = sum(p.numel() for p in model_standard.parameters() if p.requires_grad)\n",
    "\n",
    "# Load the model's state_dict\n",
    "state_dict = torch.load(\"{}/model_weights.pth\".format(model_dir), map_location='cpu')\n",
    "\n",
    "# If the state_dict was saved with 'module' prefix due to DataParallel\n",
    "# Remove 'module' prefix if present\n",
    "if list(state_dict.keys())[0].startswith('module'):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "model_standard.load_state_dict(state_dict)\n",
    "\n",
    "model_use = model_standard \n",
    "prediction_length = 8 \n",
    "\n",
    "# create a txt file to record the results\n",
    "if not os.path.exists('model_prediction'):\n",
    "\tos.mkdir('model_prediction')\n",
    "\n",
    "print('Transformer model loaded: total number of parameters: {}'.format(model_standard_total_params))\n",
    "print('Transformer model loaded:: total number of trainable parameters: {}'.format(model_standard_total_trainable_params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.1: Model sampling candidates\t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target of selection + number to sample (please specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the target sequence from N-terminal to C-terminal\n",
    "task_target = 'QPRTFLLK'\n",
    "\n",
    "# examples in the paper\n",
    "# task_target = 'QPRTFLLK'\n",
    "# task_target = 'LEILDITP'\n",
    "# task_target = 'TLEILDIT'\n",
    "\n",
    "\n",
    "# specify number of camdidates to sample\n",
    "num_candidates = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Candidates sampling (run & do not modify the codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total candidates sampled: 20\n",
      "number of unique top candidates successfully sampled: 10\n",
      "[['DSVALRVG' '0.0002399845834588632' 'GVRLAVSD']\n",
      " ['GAAAVLLG' '2.8616315830731764e-05' 'GLLVAAAG']\n",
      " ['GALEIRLS' '6.852956175862346e-06' 'SLRIELAG']\n",
      " ['WALVTEFR' '4.2970114009222016e-06' 'RFETVLAW']\n",
      " ['DRVVMTVS' '6.624817388001247e-07' 'SVTMVVRD']\n",
      " ['GALAYLWH' '5.005929324397584e-07' 'HWLYALAG']\n",
      " ['GAAECEVT' '2.3712441077350377e-07' 'TVECEAAG']\n",
      " ['WTLITEFR' '8.286725972084241e-08' 'RFETILTW']\n",
      " ['AALSNVAP' '3.414769267351403e-08' 'PAVNSLAA']\n",
      " ['QEIGLEVG' '2.9076515417614246e-08' 'GVELGIEQ']]\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20\n",
    "\n",
    "peptide_candidates = sample_candidates(model_use, task_target, num_candidates, amino_dict, prediction_length + 2, device, max_iter=max_iter)\n",
    "# add a reverse column if antiparallel\n",
    "sythesis_pep = np.array([string[::-1] for string in peptide_candidates[:, 0]])\n",
    "peptide_candidates = np.concatenate((peptide_candidates, sythesis_pep.reshape(-1, 1)), axis=1)\n",
    "\n",
    "print('The first 10 examples candidates are:')\n",
    "print('raw sequence, probability, designed complementary peptide') \n",
    "print(peptide_candidates[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save peptide candidates as a txt file in a model prediction folder\n",
    "if not os.path.exists('model_prediction'):\n",
    "\tos.mkdir('model_prediction')\n",
    "with open('model_prediction/{}_{}candidates.txt'.format(task_target, num_candidates), 'w') as f:\n",
    "\tfor i in range(len(peptide_candidates)):\n",
    "\t\tf.write(peptide_candidates[i][0] + '\\t' + str(peptide_candidates[i][1]) + '\\t' + str(peptide_candidates[i][2]) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.2: Model greedy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_comple_peptide_pred, dec_prob, dec_attention_weight_seq = predict_greedy_single(model_use, task_target, amino_dict, prediction_length + 2, device, save_attention_weights=True, print_info=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.3: Model random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_candidates = sample_single_candidate(model_use, task_target, amino_dict, prediction_length + 2, device)\n",
    "print(peptide_candidates)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2.4: Model peptides pair evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probability at position 1 is 0.6366061568260193\n",
      "Conditional probability at position 2 is 0.5831637382507324\n",
      "Conditional probability at position 3 is 0.3184937834739685\n",
      "Conditional probability at position 4 is 0.2873428463935852\n",
      "Conditional probability at position 5 is 0.18387174606323242\n",
      "Conditional probability at position 6 is 0.783111035823822\n",
      "Conditional probability at position 7 is 0.8732264637947083\n",
      "Conditional probability at position 8 is 0.351533442735672\n",
      "Input target sequence is TLEILDIT, complementary peptide is IVRVRKIL\n",
      "Evaluated probability is 0.0015017394027401424\n"
     ]
    }
   ],
   "source": [
    "complementary_peptide = 'IVRVRKIL' # note this complementary peptide is in a face to face orientation to the target sequence\n",
    "\n",
    "dec_prob, dec_attention_weight_seq = evaluate_single(model_use, task_target, complementary_peptide, amino_dict, prediction_length + 2, device, save_attention_weights=True, print_info=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Analyze the sampled results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### please specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_target = 'QPRTFLLK'\n",
    "num_candidates = 10000\n",
    "# number_to_analyze\n",
    "num_analysis = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read save data for analysis (run & do not modify the codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "train_dict = np.load('train_l8_anti/train_dict_fold0.npy', allow_pickle=True)\n",
    "train_dict = train_dict.tolist()\n",
    "train_list = []\n",
    "for target, value_dict in train_dict.items():\n",
    "    for comp, count in value_dict.items():\n",
    "        train_list.append([target, comp, count])\n",
    "train_array = np.array(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read peptide candidates from a txt file in a model prediction folder\n",
    "with open('model_prediction/{}_{}candidates.txt'.format(task_target, num_candidates), 'r') as f:\n",
    "    peptide_candidates = []\n",
    "    for line in f:\n",
    "        peptide_candidates.append(line.strip().split('\\t'))\n",
    "peptide_candidates_all = np.array(peptide_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional for clusterings\n",
    "import scipy.spatial.distance as ssd\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# optional for clusterings\n",
    "def compute_clustering_labels(peptide_candidates, amino_dict, optimal_clusters=2):\n",
    "    peptide_candidates_num, peptide_candidates_num_copy = seq2num(peptide_candidates, peptide_candidates, amino_dict)\n",
    "\n",
    "    # Calculate the pairwise Hamming distance matrix\n",
    "    condensed_distance_matrix = ssd.pdist(peptide_candidates_num, 'hamming')\n",
    "    distance_matrix = ssd.squareform(condensed_distance_matrix)\n",
    "\n",
    "    # Compute clustering labels\n",
    "    kmedoids = KMedoids(n_clusters=optimal_clusters, init='k-medoids++', random_state=42, metric='precomputed')\n",
    "    cluster_labels = kmedoids.fit_predict(distance_matrix)\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels_all = compute_clustering_labels(peptide_candidates_all[:, 0], amino_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptide_candidates_analysis = peptide_candidates_all[:num_analysis]\n",
    "# optional for clusterings\n",
    "cluster_labels_analysis = cluster_labels_all[:num_analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import percentileofscore\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_net_charge(peptide_list, reference_list=None):\n",
    "    amino_positive = ['R', 'K', 'H']\n",
    "    amino_negative = ['D', 'E']\n",
    "    charge_list = []\n",
    "\n",
    "    for peptide in peptide_list:\n",
    "        charge = 0\n",
    "        for amino_acid in peptide:\n",
    "            if amino_acid in amino_positive:\n",
    "                charge += 1\n",
    "            elif amino_acid in amino_negative:\n",
    "                charge -= 1\n",
    "        charge_list.append(charge)\n",
    "\n",
    "    if reference_list is not None:\n",
    "        reference_charge_list = [calculate_net_charge([ref_seq])[0] for ref_seq in reference_list]\n",
    "        percentile_list = [percentileofscore(reference_charge_list, charge) for charge in charge_list]\n",
    "        return charge_list, percentile_list\n",
    "    else:\n",
    "        return charge_list\n",
    "    \n",
    "kyte_doolittle_scale = {\n",
    "    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "}\n",
    "\n",
    "def calculate_hydrophobicity(peptide_list, scale, reference_list=None):\n",
    "    hydrophobicity_list = []\n",
    "\n",
    "    for peptide in peptide_list:\n",
    "        hydrophobicity = np.mean([scale[residue] for residue in peptide if residue in scale])\n",
    "        hydrophobicity_list.append(hydrophobicity)\n",
    "\n",
    "    if reference_list is not None:\n",
    "        reference_hydrophobicity_list = calculate_hydrophobicity(reference_list, scale)\n",
    "        percentile_list = [percentileofscore(reference_hydrophobicity_list, hydrophobicity) for hydrophobicity in hydrophobicity_list]\n",
    "        return hydrophobicity_list, percentile_list\n",
    "    else:\n",
    "        return hydrophobicity_list\n",
    "    \n",
    "molecular_weights = {\n",
    "    'A': 89.094,   # Alanine\n",
    "    'R': 174.203,  # Arginine\n",
    "    'N': 132.119,  # Asparagine\n",
    "    'D': 133.104,  # Aspartic acid\n",
    "    'C': 121.154,  # Cysteine\n",
    "    'E': 147.131,  # Glutamic acid\n",
    "    'Q': 146.146,  # Glutamine\n",
    "    'G': 75.067,   # Glycine\n",
    "    'H': 155.156,  # Histidine\n",
    "    'I': 131.175,  # Isoleucine\n",
    "    'L': 131.175,  # Leucine\n",
    "    'K': 146.189,  # Lysine\n",
    "    'M': 149.208,  # Methionine\n",
    "    'F': 165.192,  # Phenylalanine\n",
    "    'P': 115.132,  # Proline\n",
    "    'S': 105.093,  # Serine\n",
    "    'T': 119.120,  # Threonine\n",
    "    'W': 204.228,  # Tryptophan\n",
    "    'Y': 181.191,  # Tyrosine\n",
    "    'V': 117.148,  # Valine\n",
    "}\n",
    "\n",
    "def calculate_molecular_weights(peptide_list, scale, reference_list=None):\n",
    "    molecular_weight_list = []\n",
    "\n",
    "    for peptide in peptide_list:\n",
    "        molecular_weight = np.mean([scale[residue] for residue in peptide if residue in scale])\n",
    "        molecular_weight_list.append(molecular_weight)\n",
    "\n",
    "    if reference_list is not None:\n",
    "        reference_molecular_weight_list = calculate_molecular_weights(reference_list, scale)\n",
    "        percentile_list = [percentileofscore(reference_molecular_weight_list, molecular_weight) for molecular_weight in molecular_weight_list]\n",
    "        return molecular_weight_list, percentile_list\n",
    "    else:\n",
    "        return molecular_weight_list\n",
    "    \n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "def calculate_isoelectric_points(peptide_list, reference_list=None):\n",
    "    isoelectric_point_list = []\n",
    "\n",
    "    for peptide in peptide_list:\n",
    "        analysis = ProteinAnalysis(peptide)\n",
    "        isoelectric_point = analysis.isoelectric_point()\n",
    "        isoelectric_point_list.append(isoelectric_point)\n",
    "\n",
    "    if reference_list is not None:\n",
    "        reference_isoelectric_point_list = calculate_isoelectric_points(reference_list)\n",
    "        percentile_list = [percentileofscore(reference_isoelectric_point_list, isoelectric_point) for isoelectric_point in isoelectric_point_list]\n",
    "        return isoelectric_point_list, percentile_list\n",
    "    else:\n",
    "        return isoelectric_point_list\n",
    "    \n",
    "def calculate_aromaticity(peptide_list, reference_list=None):\n",
    "    aromaticity_list = []\n",
    "\n",
    "    for peptide in peptide_list:\n",
    "        analysis = ProteinAnalysis(peptide)\n",
    "        aromaticity = analysis.aromaticity()\n",
    "        aromaticity_list.append(aromaticity)\n",
    "\n",
    "    if reference_list is not None:\n",
    "        reference_aromaticity_list = calculate_aromaticity(reference_list)\n",
    "        percentile_list = [percentileofscore(reference_aromaticity_list, aromaticity) for aromaticity in aromaticity_list]\n",
    "        return aromaticity_list, percentile_list\n",
    "    else:\n",
    "        return aromaticity_list\n",
    "    \n",
    "def calculate_novelty_scores(peptide_list, reference_list):\n",
    "    novelty_scores = []\n",
    "\n",
    "    for peptide_search in peptide_list:\n",
    "        min_dist = 100\n",
    "        for train_peptide in reference_list:\n",
    "            for j in range(len(peptide_search)):\n",
    "                dist = 0\n",
    "                for k in range(len(peptide_search)):\n",
    "                    if peptide_search[k] != train_peptide[k]:\n",
    "                        dist += 1\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "        novelty_scores.append(min_dist)\n",
    "\n",
    "    return np.array(novelty_scores)\n",
    "\n",
    "def calculate_novelty_scores_and_reference_targets(peptide_list, reference_list, target, target_reference_list):\n",
    "    novelty_scores = []\n",
    "    target_novelty_scores = []\n",
    "\n",
    "    for peptide_search in peptide_list:\n",
    "        min_dist = 100\n",
    "        min_dist_indices = []\n",
    "        for idx, train_peptide in enumerate(reference_list):\n",
    "            dist = 0\n",
    "            for k in range(len(peptide_search)):\n",
    "                if peptide_search[k] != train_peptide[k]:\n",
    "                    dist += 1\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_dist_indices = [idx]\n",
    "            elif dist == min_dist:\n",
    "                min_dist_indices.append(idx)\n",
    "\n",
    "        novelty_scores.append(min_dist)\n",
    "        \n",
    "        min_target_dist = 100\n",
    "        for idx in min_dist_indices:\n",
    "            reference_target = target_reference_list[idx]\n",
    "            target_dist = sum(a != b for a, b in zip(target, reference_target))\n",
    "            if target_dist < min_target_dist:\n",
    "                min_target_dist = target_dist\n",
    "\n",
    "        target_novelty_scores.append(min_target_dist)\n",
    "\n",
    "    return np.array(novelty_scores), np.array(target_novelty_scores)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_output_table(peptide_candidates, peptide_candidates_prob, reference_list, output_file='output_table.xlsx', cluster_labels='', target = None, target_reference_list = None, target_novelty_score = ''):\n",
    "    print('Clustering analysis Done')\n",
    "    ranks = np.arange(1, len(peptide_candidates) + 1) \n",
    "    print('rank analysis Done')\n",
    "    charge, charge_percentile = calculate_net_charge(peptide_candidates, reference_list)\n",
    "    print('Net Charge Analysis Done')\n",
    "    hydrophobicity, hydrophobicity_percentile = calculate_hydrophobicity(peptide_candidates, kyte_doolittle_scale, reference_list)\n",
    "    print('Hydrophobicity Analysis Done')\n",
    "    molecular_weight, molecular_weight_percentile = calculate_molecular_weights(peptide_candidates, molecular_weights, reference_list)\n",
    "    print('Molecular Weight Analysis Done')\n",
    "    isoelectric_point, isoelectric_point_percentile = calculate_isoelectric_points(peptide_candidates, reference_list)\n",
    "    print('Isoelectric Point Analysis Done')\n",
    "    aromaticity, aromaticity_percentile = calculate_aromaticity(peptide_candidates, reference_list)\n",
    "    print('Aromaticity Analysis Done')\n",
    "    if target == None and target_reference_list == None and target_novelty_score == '':\n",
    "        novelty_score = calculate_novelty_scores(peptide_candidates, reference_list)\n",
    "        print('Novelty Score Analysis Done')\n",
    "    else:\n",
    "        novelty_score, target_novelty_score = calculate_novelty_scores_and_reference_targets(peptide_candidates, reference_list, target, target_reference_list)\n",
    "        print('Novelty Score and Target Novelty Score Analysis Done')\n",
    "    # reverse each of the sequence in peptide_candidates\n",
    "    peptide_candidates_synthesis = [peptide[::-1] for peptide in peptide_candidates]\n",
    "\n",
    "    data = {\n",
    "        'Rank': ranks,\n",
    "        'Cluster Label': cluster_labels,\n",
    "        'Target Sequence': '',\n",
    "        'Designed Complementary Peptide': peptide_candidates,\n",
    "        'Synthesis Complementary Peptide': peptide_candidates_synthesis,\n",
    "        'Probability': peptide_candidates_prob,\n",
    "        'Novelty Score': novelty_score,\n",
    "        'Target Novelty Score': target_novelty_score,\n",
    "        'CamSol Solubility Score': '',\n",
    "        'Net Charge': charge,\n",
    "        'Net Charge Percentile': charge_percentile,\n",
    "        'Hydrophobicity': hydrophobicity,\n",
    "        'Hydrophobicity Percentile': hydrophobicity_percentile,\n",
    "        'Molecular Weight': molecular_weight,\n",
    "        'Molecular Weight Percentile': molecular_weight_percentile,\n",
    "        'Isoelectric Point': isoelectric_point,\n",
    "        'Isoelectric Point Percentile': isoelectric_point_percentile,\n",
    "        'Aromaticity': aromaticity,\n",
    "        'Aromaticity Percentile': aromaticity_percentile\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering analysis Done\n",
      "rank analysis Done\n",
      "Net Charge Analysis Done\n",
      "Hydrophobicity Analysis Done\n",
      "Molecular Weight Analysis Done\n",
      "Isoelectric Point Analysis Done\n",
      "Aromaticity Analysis Done\n",
      "Novelty Score and Target Novelty Score Analysis Done\n",
      "Clustering analysis Done\n",
      "rank analysis Done\n",
      "Net Charge Analysis Done\n",
      "Hydrophobicity Analysis Done\n",
      "Molecular Weight Analysis Done\n",
      "Isoelectric Point Analysis Done\n",
      "Aromaticity Analysis Done\n",
      "Novelty Score and Target Novelty Score Analysis Done\n",
      "Clustering analysis Done\n",
      "rank analysis Done\n",
      "Net Charge Analysis Done\n",
      "Hydrophobicity Analysis Done\n",
      "Molecular Weight Analysis Done\n",
      "Isoelectric Point Analysis Done\n",
      "Aromaticity Analysis Done\n",
      "Novelty Score and Target Novelty Score Analysis Done\n"
     ]
    }
   ],
   "source": [
    "# Generate output table for QPRTFLLK\n",
    "task_target = \"QPRTFLLK\"\n",
    "output_file_name = 'model_prediction/output_analysis_{}.xlsx'.format(task_target)\n",
    "peptide_candidates = peptide_candidates_analysis[:, 0]\n",
    "peptide_candidates_prob = peptide_candidates_analysis[:, 1]\n",
    "reference_list = train_array[:, 1]\n",
    "target_reference_list = train_array[:, 0]\n",
    "generate_output_table(peptide_candidates, peptide_candidates_prob, reference_list, output_file=output_file_name, cluster_labels=cluster_labels_analysis, target=target, target_reference_list=target_reference_list,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
