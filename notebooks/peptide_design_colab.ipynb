{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBeta: An Introduction\n",
    "\n",
    "TransformerBeta is a generative model based on Transformer architecture, developed to generate complementary binder for linear peptide epitopes of length 8 in an antiparallel beta strand conformation. The model is trained on a curated dataset of length 8 antiparallel beta strand pairs from the AF2 Beta Strand Database.\n",
    "\n",
    "![TransformerBeta.png](data:image/png;base64,iVBO)\n",
    "\n",
    "This Google Colaboratory notebook serves as an accessible user interface for TransformerBeta. It allows users to effortlessly generate, predict, and evaluate length 8 antiparallel beta interactions using the TransformerBeta model.\n",
    "\n",
    "For detailed insight into TransformerBeta, we suggest referring to our research paper (yet to be released). \n",
    "\n",
    "To install TransformerBeta locally for your projects, visit: [TransformerBeta on Github](https://github.com/HZ3519/TransformerBeta). \n",
    "\n",
    "For individual usage of the AF2 Beta Strand Database, check out: [AF2 Beta Strand Database on Huggingface](https://huggingface.co/datasets/hz3519/AF2_Beta_Strand_Database/tree/main). \n",
    "\n",
    "For accessing the model weights and corresponding training, validation, and test sets mentioned in the paper, refer to: [TransformerBeta on Huggingface](https://huggingface.co/hz3519/TransformerBeta).\n",
    "\n",
    "## Outline of the Notebook\n",
    "1. Section 1: Setup Guidance\n",
    "2. Section 2: Model Loading\n",
    "3. Section 3: Generation of Peptide Sequences (4 different methods: 1. Iterative sampling of N unique peptides given a target sequence, 2. Random sampling of a peptide given a target sequence, 3. Greedy prediction of a peptide given a target sequence, 4. Evaluation of beta conformation probability given both the target and the peptide sequence)\n",
    "4. Section 4: Analysis of Iterative Sampling Results (if you choose for this method in Section 3)\n",
    "\n",
    "## Notebook Usage Instructions\n",
    "Please follow the instructions for each section to run the notebook successfully.\n",
    "\n",
    "## Licensing of TransformerBeta\n",
    "\n",
    "TransformerBeta, inclusive of the model weights, training set, validation set, and test set, along with the AF2 Beta Strand Database, is distributed under the terms of the [MIT License](https://opensource.org/licenses/MIT).\n",
    "\n",
    "**Kindly ensure that you adhere to the conditions stipulated by these licenses when using these files.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup Guidance\n",
    "\n",
    "Please **strictly follow** the instructions provided in each cell (Do through **Step** in order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setting up Google Drive Connection\n",
    "#@markdown To run TransformerBeta, a connection with your Google Drive is essential. This allows the program to save and access the necessary files.\n",
    "\n",
    "#@markdown **Step 1**: Execute this cell by pressing `Ctrl+Enter` or by clicking the **Play** button to the left.\n",
    "\n",
    "# This chunk will mount Google Drive to allow the storage and retrieval of files\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "drive.mount('/content/gdrive') # This path is where all output will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Installing Dependencies (this cell takes approx 17mins)\n",
    "#@markdown Ensure your runtime environment is set to CPU. \n",
    "\n",
    "#@markdown **Step 2**: Navigate to `Runtime --> Change runtime type` in the menu above and set the Hardware Accelerator to 'None'.\n",
    "\n",
    "#@markdown **Step 3**: Execute this cell to install the necessary dependencies by pressing `Ctrl+Enter` or by clicking the **Play** button to the left. \n",
    "\n",
    "# This chunk will install the necessary dependencies\n",
    "!pip install d2l==0.17.5\n",
    "!pip install pandas==1.4.3\n",
    "!pip install biopython==1.81\n",
    "!pip install scikit-learn-extra==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Restarting Runtime and reconnecting Google Drive\n",
    "#@markdown Once the installation of the necessary packages is completed, a restart of the runtime is required for the changes to take effect.\n",
    "#@markdown **Step 4**: Navigate to `Runtime --> Restart runtime` in the menu above to restart.\n",
    "\n",
    "#@markdown After-restart, the connection to Google Drive needs to be reconnected.\n",
    "#@markdown **Step 5**: Execute this cell again by pressing `Ctrl+Enter` or by clicking the **Play** button to the left to reconnect Google Drive.\n",
    "\n",
    "# reconnect to Google drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install the Github TransformerBeta package \n",
    "#@markdown **Step 6**: Execute this cell by pressing `Ctrl+Enter` or by clicking the **Play** button to the left to install the TransformerBeta package.\n",
    "\n",
    "# remove TransformerBeta_project if it exists\n",
    "import shutil\n",
    "try:\n",
    "  shutil.rmtree('/content/TransformerBeta_project', ignore_errors=True)\n",
    "except:\n",
    "  print('')\n",
    "\n",
    "# personal token for testing repo: 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\n",
    "# if github repo released public\n",
    "# !git clone https://github.com/HZ3519/TransformerBeta_project.git\n",
    "# personal token for testing repo: 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\n",
    "!git clone https://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@github.com/HZ3519/TransformerBeta_project.git\n",
    "\n",
    "!pip install -e ./TransformerBeta_project\n",
    "import sys\n",
    "if '/content/TransformerBeta_project/' not in sys.path:\n",
    "    sys.path.append('/content/TransformerBeta_project/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown The default model is \"model M retrain\", current best performing model from the paper. Execute this cell by pressing `Ctrl+Enter` or by clicking the **Play** button to the left.\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from TransformerBeta import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_name = \"model_M_retrain\" #@param {type:\"string\"}\n",
    "\n",
    "# git clone the model directory from huggerface\n",
    "!git lfs install\n",
    "# if the hugging face repo is public, change it:\n",
    "# !git clone https://huggingface.co/hz3519/TransformerBeta_models\n",
    "# personal token for testing repo: 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\n",
    "!git clone https://hz3519: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@huggingface.co/hz3519/TransformerBeta_models\n",
    "\n",
    "model_dir = \"TransformerBeta_models/{}\".format(model_name)\n",
    "\n",
    "# Load the config\n",
    "with open(\"{}/config.json\".format(model_dir), \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create instances of your encoder and decoder\n",
    "encoder_standard = TransformerEncoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"])\n",
    "decoder_standard = TransformerDecoder(\n",
    "    config[\"vocab_size\"], config[\"key_size\"], config[\"query_size\"], config[\"value_size\"], config[\"num_hiddens\"], \n",
    "    config[\"norm_shape\"], config[\"ffn_num_input\"], config[\"ffn_num_hiddens\"], config[\"num_heads\"],\n",
    "    config[\"num_layers\"], config[\"dropout\"], shared_embedding=encoder_standard.embedding)\n",
    "\n",
    "# Create an instance of your model\n",
    "model_standard = EncoderDecoder(encoder_standard, decoder_standard)\n",
    "model_standard_total_params = sum(p.numel() for p in model_standard.parameters())\n",
    "model_standard_total_trainable_params = sum(p.numel() for p in model_standard.parameters() if p.requires_grad)\n",
    "\n",
    "# Load the model's state_dict\n",
    "state_dict = torch.load(\"{}/model_weights.pth\".format(model_dir), map_location='cpu')\n",
    "\n",
    "# If the state_dict was saved with 'module' prefix due to DataParallel\n",
    "# Remove 'module' prefix if present\n",
    "if list(state_dict.keys())[0].startswith('module'):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove 'module'\n",
    "        new_state_dict[name] = v\n",
    "    state_dict = new_state_dict\n",
    "    \n",
    "model_standard.load_state_dict(state_dict)\n",
    "\n",
    "model_use = model_standard \n",
    "prediction_length = 8\n",
    "device = d2l.try_gpu()\n",
    "\n",
    "output_dir = \"gdrive/MyDrive/model_prediction_{}\".format(model_name)\n",
    "# save peptide candidates as a txt file in a model prediction folder\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.mkdir(output_dir)\n",
    "\n",
    "print('Transformer model loaded: total number of parameters: {}'.format(model_standard_total_params))\n",
    "print('Transformer model loaded:: total number of trainable parameters: {}'.format(model_standard_total_trainable_params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Generation of Peptide Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Iterative Sampling\n",
    "#@markdown please specify the length 8 target sequence from N-terminal to C-terminal\n",
    "Target = 'QPRTFLLK' #@param {type:\"string\"}\n",
    "#@markdown specify number of camdidates to sample\n",
    "num_candidates = 10 #@param {type:\"integer\"}\n",
    "#@markdown After running the code, the results can be accessed either directly from your Google Drive or from the left sidebar of this Google Colab notebook at path `gdrive/MyDrive/model_prediction_{model_name}/{Target}_{num_candidates}candidates.txt`\n",
    "\n",
    "#@markdown **Note:** Please only sample 100 candidates maximum at a time in Google Colab. If you want to sample more candidates, please install the project locally and run the code locally with enough memory. Sample time for 100 candidates is ~11 sec.\n",
    "\n",
    "max_iter = 20\n",
    "peptide_candidates = sample_candidates(model_use, Target, num_candidates, amino_dict, prediction_length + 2, device, max_iter=max_iter)\n",
    "# add a reverse column if antiparallel\n",
    "sythesis_pep = np.array([string[::-1] for string in peptide_candidates[:, 0]])\n",
    "peptide_candidates = np.concatenate((peptide_candidates, sythesis_pep.reshape(-1, 1)), axis=1)\n",
    "\n",
    "print('The first 10 examples candidates are:')\n",
    "print('raw sequence, probability, designed complementary peptide') \n",
    "print(peptide_candidates[:10])\n",
    "\n",
    "with open('{}/{}_{}candidates.txt'.format(output_dir, Target, num_candidates), 'w') as f:\n",
    "\tfor i in range(len(peptide_candidates)):\n",
    "\t\tf.write(peptide_candidates[i][0] + '\\t' + str(peptide_candidates[i][1]) + '\\t' + str(peptide_candidates[i][2]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Random Sampling\n",
    "#@markdown please specify the length 8 target sequence from N-terminal to C-terminal\n",
    "Target = 'QPRTFLLK' #@param {type:\"string\"}\n",
    "\n",
    "peptide_candidates = sample_single_candidate(model_use, Target, amino_dict, prediction_length + 2, device)\n",
    "print(peptide_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Greedy Prediction\n",
    "#@markdown please specify the length 8 target sequence from N-terminal to C-terminal\n",
    "Target = 'QPRTFLLK' #@param {type:\"string\"}\n",
    "\n",
    "dec_comple_peptide_pred, dec_prob, dec_attention_weight_seq = predict_greedy_single(model_use, Target, amino_dict, prediction_length + 2, device, save_attention_weights=True, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Evaluation of beta conformation probability\n",
    "#@markdown please specify the length 8 target sequence from N-terminal to C-terminal\n",
    "Target = 'QPRTFLLK' #@param {type:\"string\"}\n",
    "#@markdown please specify the length 8 complementary peptide from C-terminal to N-terminal (face to face orientation)\n",
    "complementary_peptide = 'IVRVRKIL' #@param {type:\"string\"}\n",
    "\n",
    "dec_prob, dec_attention_weight_seq = evaluate_single(model_use, Target, complementary_peptide, amino_dict, prediction_length + 2, device, save_attention_weights=True, print_info=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Analysis of Iterative Sampling Results (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load the sampled results\n",
    "#@markdown please specify the length 8 target sequence from N-terminal to C-terminal\n",
    "Target = 'QPRTFLLK' #@param {type:\"string\"}\n",
    "#@markdown specify number of camdidates sampled\n",
    "num_candidates = 10 #@param {type:\"integer\"}\n",
    "#@markdown specify the number of candidates to analyze\n",
    "num_analysis = 5 #@param {type:\"integer\"}\n",
    "#@markdown specify whether to use training data as a reference\n",
    "use_train = False #@param {type:\"boolean\"}\n",
    "\n",
    "# read peptide candidates from a txt file in a model prediction folder\n",
    "with open('{}/{}_{}candidates.txt'.format(output_dir, Target, num_candidates), 'r') as f:\n",
    "    peptide_candidates = []\n",
    "    for line in f:\n",
    "        peptide_candidates.append(line.strip().split('\\t'))\n",
    "peptide_candidates_all = np.array(peptide_candidates)\n",
    "\n",
    "# load training data\n",
    "if use_train:\n",
    "\ttrain_dict = np.load('{}/train_data.npy'.format(model_dir), allow_pickle=True)\n",
    "\ttrain_dict = train_dict.tolist()\n",
    "\ttrain_list = []\n",
    "\tfor target, value_dict in train_dict.items():\n",
    "\t\tfor comp, count in value_dict.items():\n",
    "\t\t\ttrain_list.append([target, comp, count])\n",
    "\ttrain_array = np.array(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Analyze the sampled results\n",
    "\n",
    "# compute the cluster labels\n",
    "cluster_labels_all = compute_clustering_labels(peptide_candidates_all[:, 0], amino_dict)\n",
    "peptide_candidates_analysis = peptide_candidates_all[:num_analysis]\n",
    "cluster_labels_analysis = cluster_labels_all[:num_analysis]\n",
    "\n",
    "# Generate output table for\n",
    "output_file_name = '{}/output_analysis_{}_{}_{}.xlsx'.format(output_dir, Target, num_candidates, num_analysis)\n",
    "peptide_candidates = peptide_candidates_analysis[:, 0]\n",
    "peptide_candidates_prob = peptide_candidates_analysis[:, 1]\n",
    "if use_train:\n",
    "\treference_list = train_array[:, 1]\n",
    "\ttarget_reference_list = train_array[:, 0]\n",
    "else:\n",
    "\treference_list = None\n",
    "\ttarget_reference_list = None\n",
    "generate_output_table(peptide_candidates, peptide_candidates_prob, reference_list, output_file=output_file_name, cluster_labels=cluster_labels_analysis, target=target, target_reference_list=target_reference_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
